{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taimoorrauf607/Practice-/blob/master/ML_cheatsheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b571cc6-f427-43da-a175-19ea44835665",
      "metadata": {
        "id": "0b571cc6-f427-43da-a175-19ea44835665"
      },
      "source": [
        "# Fetaure Engineering\n",
        "#### 1. feature scalling\n",
        "#### 2. Fetaure transformation\n",
        "#### 3. feature construction\n",
        "#### 4. feature selecting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37bd1c60-856a-4159-9999-f69bbf731d74",
      "metadata": {
        "id": "37bd1c60-856a-4159-9999-f69bbf731d74"
      },
      "source": [
        "## 1. Feature Scaliing\n",
        "Adjusting features to a common scale without distorting differences in the ranges of values.\n",
        "#### Key Types:\n",
        "#### Normalization: Scales values between 0 and 1.\n",
        "#### Standardization: Scales values to have zero mean and unit variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0df351-1234-48ed-abfe-32057a0309b3",
      "metadata": {
        "id": "fd0df351-1234-48ed-abfe-32057a0309b3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6a3da0-c810-4702-80af-4b01b4dffdce",
      "metadata": {
        "id": "bd6a3da0-c810-4702-80af-4b01b4dffdce"
      },
      "source": [
        "## 2. Feature Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8b8c7ca-9a05-4a49-b682-ee2b7341a1ef",
      "metadata": {
        "id": "a8b8c7ca-9a05-4a49-b682-ee2b7341a1ef"
      },
      "source": [
        "#### Ordinal Encoding\n",
        "Definition: Ordinal encoding is used to convert categorical data into numerical\n",
        "format while preserving the order of categories. It is particularly useful\n",
        "for ordinal data (data with an inherent order, e.g., \"low\", \"medium\", \"high\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f737359-aaef-4cca-b10d-3eb34b47471f",
      "metadata": {
        "id": "1f737359-aaef-4cca-b10d-3eb34b47471f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "encoder = OrdinalEncoder(categories=[['low', 'medium', 'high']])  # Define order explicitly if needed\n",
        "X_encoded = encoder.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc450222-e767-421d-a57d-9e9f57ddbc51",
      "metadata": {
        "id": "cc450222-e767-421d-a57d-9e9f57ddbc51"
      },
      "source": [
        "### Label Encoding\n",
        "Definition: Label encoding is a technique to convert categorical labels into numerical values.\n",
        "Each unique category is assigned an integer value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51f0d0d4-909a-4a58-924d-b4ca409d7529",
      "metadata": {
        "id": "51f0d0d4-909a-4a58-924d-b4ca409d7529"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5fc296-b507-4906-b410-db2d574698e0",
      "metadata": {
        "id": "8a5fc296-b507-4906-b410-db2d574698e0"
      },
      "source": [
        "### One-hot encoding\n",
        "Definition: One-hot encoding converts categorical variables into binary columns, with each unique category represented as\n",
        "a separate column containing 1 for the category it belongs to and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26ee97f-3aa7-4c71-86bc-16bd3f5a0f2b",
      "metadata": {
        "id": "e26ee97f-3aa7-4c71-86bc-16bd3f5a0f2b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)  # Use sparse=True for sparse matrix\n",
        "X_encoded = encoder.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "273234ce-e71e-481a-b3fe-868fe76cc34f",
      "metadata": {
        "id": "273234ce-e71e-481a-b3fe-868fe76cc34f"
      },
      "source": [
        "### Column Transformer\n",
        "Definition: The ColumnTransformer applies different preprocessing transformations to specified columns of a dataset,\n",
        "enabling efficient handling of mixed data types (e.g., numerical and categorical features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066a7143-c652-4213-b288-be1b411e091c",
      "metadata": {
        "id": "066a7143-c652-4213-b288-be1b411e091c"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "transformer = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), numerical_columns),\n",
        "    ('cat', OneHotEncoder(), categorical_columns)\n",
        "], remainder='passthrough')\n",
        "X_transformed = transformer.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182e1257-b2e4-41a8-ae1d-d988e9197d3c",
      "metadata": {
        "id": "182e1257-b2e4-41a8-ae1d-d988e9197d3c"
      },
      "source": [
        "### pipelines\n",
        "Definition: A pipeline is a way to streamline workflows in machine learning by chaining multiple steps, such as preprocessing, feature selection, and modeling, into a single object. It ensures that the transformations are applied in sequence, and avoids data leakage by fitting the model and preprocessing steps together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b58d416-c890-463c-ad91-c9a4b8cdca01",
      "metadata": {
        "id": "3b58d416-c890-463c-ad91-c9a4b8cdca01"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the steps of the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "# Fit the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "# Predict using the pipeline\n",
        "y_pred = pipeline.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e7ee98e-76ab-425e-98f2-f9b4f9db9f55",
      "metadata": {
        "id": "4e7ee98e-76ab-425e-98f2-f9b4f9db9f55"
      },
      "source": [
        "### Mathematical Transformation\n",
        "Definition: Mathematical transformations modify feature values using mathematical functions like logarithms, square roots, or exponents to reduce skewness, stabilize variance, or make features more suitable for modeling.\n",
        "### Log Transformation\n",
        "Definition: Log transformation reduces the impact of extreme values by applying the logarithm function to the data, often used for positively skewed distributions.\n",
        "### Power Transformation\n",
        "Definition: Power transformation (e.g., Box-Cox or Yeo-Johnson) applies a mathematical power function to stabilize variance and make data more Gaussian-like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2a2afe-4f12-4c32-859e-3dc20db49056",
      "metadata": {
        "id": "7e2a2afe-4f12-4c32-859e-3dc20db49056"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "import numpy as np\n",
        "\n",
        "X_log_transformed = np.log1p(X)  # log1p handles zero values\n",
        "pt = PowerTransformer(method='yeo-johnson')  # or 'box-cox'\n",
        "X_power_transformed = transformer.fit_transform(X)\n",
        "pt.lambdas_  # check lambdas values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae4e56af-651e-4051-aa30-f799a8e82dd7",
      "metadata": {
        "id": "ae4e56af-651e-4051-aa30-f799a8e82dd7"
      },
      "source": [
        "### Discretization (Binning)\n",
        "Definition: Discretization, or binning, is the process of transforming continuous features into discrete bins or intervals to simplify the data and capture important patterns.\n",
        "\n",
        "#### Strategies:\n",
        "\n",
        "#### Uniform Binning: Divides the range of data into equally spaced intervals.\n",
        "#### Quantile Binning: Divides data into intervals containing an equal number of data points.\n",
        "#### KMeans Binning: Uses clustering (K-means) to determine the bin edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa88dc6-b3b9-45fe-a166-43ae9499b110",
      "metadata": {
        "id": "cfa88dc6-b3b9-45fe-a166-43ae9499b110"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')  # 'quantile' or 'kmeans' are other strategies\n",
        "X_binned = discretizer.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af1ed2e-2816-4729-a1f8-940b5943a783",
      "metadata": {
        "id": "9af1ed2e-2816-4729-a1f8-940b5943a783"
      },
      "source": [
        "### Binarization\n",
        "Definition: Binarization transforms numerical values into binary values (0 or 1) based on a specified threshold.\n",
        "Values above the threshold are mapped to 1, and values below or equal to the threshold are mapped to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc5a257-e29b-4b48-ad1b-3457d3f4d593",
      "metadata": {
        "id": "0bc5a257-e29b-4b48-ad1b-3457d3f4d593"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "binarizer = Binarizer(copy=False)\n",
        "X_binarized = binarizer.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c87474-dc54-44fa-9932-9f02b59dbdab",
      "metadata": {
        "id": "78c87474-dc54-44fa-9932-9f02b59dbdab"
      },
      "source": [
        "## Handle Missing Values\n",
        "Definition: Handling missing values involves techniques to either fill in or remove missing data to prevent errors in model training.\n",
        "\n",
        "### Strategies:\n",
        "\n",
        "Imputation: Fill missing values with a specific strategy (mean, median, mode, constant value).\n",
        "Deletion: Remove rows or columns containing missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a634c156-6764-425f-a655-74c4af97e4be",
      "metadata": {
        "id": "a634c156-6764-425f-a655-74c4af97e4be"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean', add_indicator=True)  # Use 'median', 'most_frequent', or 'constant' as needed\n",
        "X_imputed = imputer.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd261097-6850-45a4-979b-ec80cad98716",
      "metadata": {
        "id": "cd261097-6850-45a4-979b-ec80cad98716"
      },
      "source": [
        "### Multivariate Imputer\n",
        "Definition: A multivariate imputer uses relationships between features to predict and\n",
        "impute missing values based on other available features.\n",
        "### KNN Imputer\n",
        "Definition: The KNN imputer fills missing values by considering the nearest neighbors of the missing data points,\n",
        "    using K-nearest neighbors for imputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e08df4-d05c-4339-9afa-5f4c5a9a0485",
      "metadata": {
        "id": "52e08df4-d05c-4339-9afa-5f4c5a9a0485"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import IterativeImputer,KNNImputer\n",
        "\n",
        "imputer1 = IterativeImputer(max_iter=50,n_nearest_features=2)\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "imputer2 = KNNImputer(n_neighbors=5)\n",
        "X_imputed = imputer.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b97cdad-6c9e-4e8c-bac1-a00eebccd160",
      "metadata": {
        "id": "0b97cdad-6c9e-4e8c-bac1-a00eebccd160"
      },
      "source": [
        "### Outlier Treatment Strategies\n",
        "Outliers can significantly affect the performance of machine learning models. Below are strategies to deal with them:\n",
        "#### How to detect outliers?\n",
        "1. in Normal distribution case : (μ+ 3σ) > , (μ- 3σ)<\n",
        "2. in Skewed distribution case: Interqurtile Range  min = Q1 - 1.5*iqr   , max = q3+1.5*iqr\n",
        "3. other distribution case : 2.5 percentil to 97.5 percentile"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76390f80-c333-4b85-8073-fbff192d767d",
      "metadata": {
        "id": "76390f80-c333-4b85-8073-fbff192d767d"
      },
      "source": [
        "### 1. Trimming\n",
        "Definition: Trimming involves removing the outliers entirely from the dataset. This is done by defining thresholds and excluding any data points that fall outside of these boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6c48e4f-ffad-43d7-83fb-3ad246fa1bb4",
      "metadata": {
        "id": "e6c48e4f-ffad-43d7-83fb-3ad246fa1bb4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define upper and lower thresholds\n",
        "lower_threshold = np.percentile(X, 5)\n",
        "upper_threshold = np.percentile(X, 95)\n",
        "\n",
        "X_trimmed = X[(X >= lower_threshold) & (X <= upper_threshold)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b453bb46-3389-40e5-adaa-19bfa0743529",
      "metadata": {
        "id": "b453bb46-3389-40e5-adaa-19bfa0743529"
      },
      "source": [
        "### 2. Capping (Winsorizing)\n",
        "Definition: Capping involves replacing outliers with the nearest valid data point within a defined range. This method is also known as Winsorizing.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc7a727-defb-49fd-a900-834ddafb3dd5",
      "metadata": {
        "id": "7dc7a727-defb-49fd-a900-834ddafb3dd5"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mstats\n",
        "# Cap values at the 5th and 95th percentiles\n",
        "X_capped = mstats.winsorize(X, limits=[0.05, 0.05])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0427d41-81db-4736-a14b-4266bc8c6715",
      "metadata": {
        "id": "d0427d41-81db-4736-a14b-4266bc8c6715"
      },
      "source": [
        "### 3. Missing Value-like Treatment\n",
        "Definition: Treat outliers as missing values and then apply imputation techniques to fill them. This strategy is helpful when the outliers are extreme and might distort the data too much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e9976d-e23d-423f-8812-2bf8deed52a3",
      "metadata": {
        "id": "d6e9976d-e23d-423f-8812-2bf8deed52a3"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Set a threshold for outliers\n",
        "outlier_threshold = np.percentile(X, 95)\n",
        "\n",
        "# Replace outliers with NaN\n",
        "X[X > outlier_threshold] = np.nan\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')  # Use median or other strategies as needed\n",
        "X_imputed = imputer.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "416c0bb2-3c48-4d31-95f6-7c4d5d6202d3",
      "metadata": {
        "id": "416c0bb2-3c48-4d31-95f6-7c4d5d6202d3"
      },
      "source": [
        "### 4. Interquartile Range method\n",
        "Definition: The IQR method identifies outliers by calculating the interquartile range (IQR), which is the difference between the 75th percentile (Q3) and the 25th percentile (Q1). Data points that fall outside of the range are outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c425f9-fe6f-4283-a13b-7215be2e1a32",
      "metadata": {
        "id": "26c425f9-fe6f-4283-a13b-7215be2e1a32"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Calculate Q1 and Q3\n",
        "Q1 = np.percentile(X, 25)\n",
        "Q3 = np.percentile(X, 75)\n",
        "IQR = Q3 - Q1\n",
        "# Define outlier thresholds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "# Filter out the outliers\n",
        "X_filtered = X[(X >= lower_bound) & (X <= upper_bound)]\n",
        "np.where(data>upper_bound,upper_bound,np.where(data<lower_bound,lower_bound,data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45a7735-62f8-48fe-b2c4-fdb6c18aa42c",
      "metadata": {
        "id": "d45a7735-62f8-48fe-b2c4-fdb6c18aa42c"
      },
      "source": [
        "## 3. Feature Construction\n",
        "Definition: Feature construction involves creating new features from the existing ones to improve the model's ability to learn patterns. This can be done through mathematical operations, aggregating features, or extracting domain-specific information.\n",
        "### Interaction Features\n",
        "Definition: Interaction features are created by combining two or more features to capture their combined effect on the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386ff5a0-5395-485d-a8ad-fdd97c1fb5ab",
      "metadata": {
        "id": "386ff5a0-5395-485d-a8ad-fdd97c1fb5ab"
      },
      "outputs": [],
      "source": [
        "df['new_feature'] = df['feature1'] * df['feature2']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b22fe100-f288-4903-97a1-5dc10e485ee6",
      "metadata": {
        "id": "b22fe100-f288-4903-97a1-5dc10e485ee6"
      },
      "source": [
        "### Polynomial Features\n",
        "Definition: Polynomial features are higher-order terms (e.g., squares, cubes) created from numerical features to capture non-linear relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b865ae6-9275-4dda-8c93-16ef46db054e",
      "metadata": {
        "id": "0b865ae6-9275-4dda-8c93-16ef46db054e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab23135-c5a2-4bc3-9283-2b15c4ae29df",
      "metadata": {
        "id": "9ab23135-c5a2-4bc3-9283-2b15c4ae29df"
      },
      "source": [
        "### Date/Time-based Features\n",
        "Definition: Extract meaningful features from date or time-related columns, such as year, month, day, day of the week, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1895f479-1302-4a29-ac87-5930777e78cf",
      "metadata": {
        "id": "1895f479-1302-4a29-ac87-5930777e78cf"
      },
      "outputs": [],
      "source": [
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4904f6b3-c7c9-4112-8b51-0841908cb651",
      "metadata": {
        "id": "4904f6b3-c7c9-4112-8b51-0841908cb651"
      },
      "source": [
        "### Binned Features\n",
        "Definition: Create categorical features by grouping continuous values into bins or categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7da39bc9-2d0d-4f06-9ec5-f4455264f4e7",
      "metadata": {
        "id": "7da39bc9-2d0d-4f06-9ec5-f4455264f4e7"
      },
      "outputs": [],
      "source": [
        "df['binned_feature'] = pd.cut(df['feature'], bins=5, labels=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1801686-5c31-471a-bfff-be4fe216d13e",
      "metadata": {
        "id": "f1801686-5c31-471a-bfff-be4fe216d13e"
      },
      "source": [
        "## 4. Feature Extraction\n",
        "### Principal Component Analysis (PCA)\n",
        "Definition: PCA reduces the dimensionality of data by projecting it onto principal components that capture the most variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2523007d-acd9-4a1b-8402-ea497a5d197d",
      "metadata": {
        "id": "2523007d-acd9-4a1b-8402-ea497a5d197d"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "# X_pca = pca.fit_transform(X)\n",
        "pca.explained_variance_  #eigen values\n",
        "pca.components_   # eigen vectors\n",
        "pca.explained_variance_ratio_   # contribution percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe826a5-ae0b-4cb3-8d02-2fa45791c431",
      "metadata": {
        "id": "cfe826a5-ae0b-4cb3-8d02-2fa45791c431"
      },
      "source": [
        "# Machine learning Types\n",
        "\n",
        "1. Supervised Learning: Trains on labeled data (e.g., classification, regression).\n",
        "2. Unsupervised Learning: Trains on unlabeled data (e.g., clustering, dimensionality reduction).\n",
        "3. Semi-supervised Learning: Mix of labeled and unlabeled data.\n",
        "4. Reinforcement Learning: Learning through rewards and penalties.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46960f09-2684-465b-b603-af9ebdce8c0a",
      "metadata": {
        "id": "46960f09-2684-465b-b603-af9ebdce8c0a"
      },
      "source": [
        "# 1. Supervised Machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4f25c1-a695-4f84-96e2-4da78403bf17",
      "metadata": {
        "id": "2e4f25c1-a695-4f84-96e2-4da78403bf17"
      },
      "source": [
        "## linear Models\n",
        "### Linear Regression\n",
        "Definition: Linear regression is a supervised learning algorithm that models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02936fc7-3518-4622-9cb7-d30a9aef651e",
      "metadata": {
        "id": "02936fc7-3518-4622-9cb7-d30a9aef651e"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "model.coef_\n",
        "model.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main Assumptions of Linear Regression**\n",
        "\n",
        "1. **Linearity**: The relationship between the independent and dependent variables is linear.\n",
        "2. **Independence**: Observations are independent of each other.\n",
        "3. **Homoscedasticity**: The variance of residuals (errors) is constant across all levels of the independent variable.\n",
        "4. **Normality of Errors**: Residuals are normally distributed.\n",
        "5. **No Multicollinearity**: Independent variables are not highly correlated with each other.\n",
        "6. **No Autocorrelation**: Residuals are not correlated (important in time series data)."
      ],
      "metadata": {
        "id": "xYDRnlA5zxAL"
      },
      "id": "xYDRnlA5zxAL"
    },
    {
      "cell_type": "markdown",
      "id": "bd69b9c1-6d29-4499-9924-419f3204af9f",
      "metadata": {
        "id": "bd69b9c1-6d29-4499-9924-419f3204af9f"
      },
      "source": [
        "## Regression Metrics\n",
        "Definition: Regression metrics evaluate the performance of regression models by quantifying the error between predicted and actual values.\n",
        "#### Mean Absolute Error (MAE)\n",
        "Measures the average absolute difference between predicted and actual values.\n",
        "#### Mean Squared Error (MSE)\n",
        "Measures the average squared difference between predicted and actual values.\n",
        "#### Root Mean Squared Error (RMSE)\n",
        "Square root of MSE, provides error in the same units as the target variable.\n",
        "#### R-Squared (Coefficient of Determination)\n",
        "Represents the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
        "#### Adjusted R-Squared\n",
        "Adjusts r2 for the number of predictors in the model, penalizing excessive use of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68766ddb-e949-424f-81a4-95eded6ea169",
      "metadata": {
        "id": "68766ddb-e949-424f-81a4-95eded6ea169"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\n",
        "import numpy as np\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "# root mean squared error\n",
        "rmse = np.sqrt(mse)\n",
        "#r2_score\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "# adjusted r2_score\n",
        "n = len(y_true)  # Number of observations\n",
        "p = X.shape[1]  # Number of predictors\n",
        "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24fde948-5164-411d-8acc-345a2e0badb5",
      "metadata": {
        "id": "24fde948-5164-411d-8acc-345a2e0badb5"
      },
      "source": [
        "## Gradient Descent\n",
        "Definition: Gradient descent is an optimization algorithm used to minimize a function by iteratively updating its parameters in the direction of the negative gradient of the loss function with respect to the parameters.\n",
        "#### Batch Gradient Descent\n",
        "Definition: Batch Gradient Descent computes the gradient of the loss function using the entire dataset and updates the model parameters in each iteration. It provides stable updates but can be computationally expensive for large datasets.\n",
        "#### Stochastic Gradient Descent (SGD)\n",
        "Definition: Stochastic Gradient Descent (SGD) is a variant of gradient descent that updates model parameters for each training example or a small batch, rather than the entire dataset. It is faster for large datasets but introduces noise in updates.\n",
        "#### Mini-Batch Gradient Descent\n",
        "Definition: Mini-Batch Gradient Descent splits the dataset into small batches and computes the gradient for each batch. It balances the efficiency of Batch Gradient Descent and the noise reduction of Stochastic Gradient Descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a43d0e8-fd9f-4518-9821-0459ce7575de",
      "metadata": {
        "id": "3a43d0e8-fd9f-4518-9821-0459ce7575de"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "# stochastic\n",
        "model = SGDRegressor(learning_rate='constant', eta0=0.01, max_iter=1000)\n",
        "model.fit(X, y)  # learning rate can be 'optimal', 'constant','adaptive'\n",
        "\n",
        "# mini_batch\n",
        "batch_size = 30\n",
        "for i in range(100):\n",
        "    idx = random.smaple(range(x_train.shape[0]),batch_size)\n",
        "    sgd.partial_fit(x_train[idx],y_train[idx])\n",
        "\n",
        "sgd.coef_\n",
        "sgd.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4d0e7e-d797-4218-863e-3315f84ecbcd",
      "metadata": {
        "id": "eb4d0e7e-d797-4218-863e-3315f84ecbcd"
      },
      "source": [
        "### Polynomial Features\n",
        "Definition: Polynomial features generate new features by raising existing numerical features to a specified power and creating interaction terms, enabling models to capture non-linear relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b7b0bc-28e8-4bf7-a3df-82da3ba2fdb9",
      "metadata": {
        "id": "d8b7b0bc-28e8-4bf7-a3df-82da3ba2fdb9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d08efd-e2d0-4cdd-8737-3a8ffaf6c27f",
      "metadata": {
        "id": "e4d08efd-e2d0-4cdd-8737-3a8ffaf6c27f"
      },
      "source": [
        "### Bias-Variance Trade-off\n",
        "Definition: The bias-variance trade-off is the balance between two sources of error in machine learning models:\n",
        "\n",
        "#### Bias: Error due to overly simplistic models that make strong assumptions and fail to capture the underlying data pattern.\n",
        "\n",
        "#### Variance: Error due to models that are too complex and sensitive to the noise in the training data.\n",
        "\n",
        "#### Low Bias, High Variance: Complex models (e.g., deep learning) can overfit the data, capturing noise as well as the signal.\n",
        "\n",
        "#### High Bias, Low Variance: Simpler models (e.g., linear regression) may underfit, not capturing the complexity of the data.\n",
        "\n",
        "#### Goal: Minimize both bias and variance to achieve good generalization, typically by selecting the right model complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02962682-91ca-48d5-bac5-9fd51523c923",
      "metadata": {
        "id": "02962682-91ca-48d5-bac5-9fd51523c923"
      },
      "source": [
        "### Regularization\n",
        "Definition: Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. This penalty discourages the model from learning overly complex patterns that might only capture noise in the data. Regularization methods aim to strike a balance between model complexity and accuracy.\n",
        "\n",
        "**Key Types:**\n",
        "#### L1 Regularization (Lasso):\n",
        "Adds the absolute value of the coefficients as a penalty term. It can shrink some coefficients to zero,it is used in performing feature selection.\n",
        "#### L2 Regularization (Ridge):\n",
        "Adds the square of the coefficients as a penalty term. It helps to reduce the magnitude of coefficients but does not set them to zero.\n",
        "#### ElasticNet:\n",
        "Combines both L1 and L2 regularization, allowing for a mix of the benefits of both Lasso and Ridge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b343bb80-90db-47f4-bf88-2f85605d757b",
      "metadata": {
        "id": "b343bb80-90db-47f4-bf88-2f85605d757b"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "# solver {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’} default=’auto’  # OLS\n",
        "# solver { ‘sparse_cg’, ‘sag’, ‘saga’, ‘lbfgs’}   # Gradient Descent\n",
        "model = Ridge(alpha=1.0, solver='lbfgs')  # Alpha is the regularization strength\n",
        "\n",
        "model = Lasso(alpha=1.0, solver='saga')  # Alpha is the regularization strength not small not so high\n",
        "\n",
        "model = ElasticNet(alpha=1.0, l1_ratio=0.5)  # l1_ratio controls the mix\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "model.coef_\n",
        "model.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fefe4880-9fbe-4247-8b70-b99f40f1eb06",
      "metadata": {
        "id": "fefe4880-9fbe-4247-8b70-b99f40f1eb06"
      },
      "source": [
        "### Logistic Regression\n",
        "Definition: Logistic regression is a statistical method used for binary classification problems. It models the probability of a binary outcome (0 or 1) as a function of the input features by using the logistic function (sigmoid) to map predictions between 0 and 1.\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’     **regualarization**\n",
        "\n",
        "solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’   **gradient descent apply**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ad0068-2707-4c89-8ee6-606daacfc1d6",
      "metadata": {
        "id": "e0ad0068-2707-4c89-8ee6-606daacfc1d6"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(solver='saga',max_iter=500,random_state=42)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcd4f12a-a906-46c6-b7d0-147550b4c571",
      "metadata": {
        "id": "fcd4f12a-a906-46c6-b7d0-147550b4c571"
      },
      "source": [
        "##  Classification Metrics\n",
        "Definition: Classification metrics evaluate the performance of a classification model by comparing predicted labels with actual labels.\n",
        "### 1. Accuracy\n",
        "Definition: The proportion of correctly classified instances to the total instances in the dataset.\n",
        "### 2. Confusion Matrix\n",
        "Definition: A table used to evaluate the performance of a classification algorithm by showing the counts of true positives, false positives, true negatives, and false negatives.\n",
        "### 3. Precision\n",
        "Definition: The proportion of correctly predicted positive instances to all instances predicted as positive. It measures the accuracy of positive predictions.\n",
        "### 4. Recall (Sensitivity)\n",
        "Definition: The proportion of correctly predicted positive instances to all actual positive instances. It measures the model's ability to identify all relevant positive cases.\n",
        "### 5. F1-Score\n",
        "Definition: The harmonic mean of precision and recall, providing a balance between them. It is useful when the class distribution is imbalanced.\n",
        "### 6. Classification Report\n",
        "Definition: A comprehensive summary of precision, recall, F1-score, and support (the number of occurrences of each class) for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1f4094-0e41-4442-b3f0-14586e8545fb",
      "metadata": {
        "id": "be1f4094-0e41-4442-b3f0-14586e8545fb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score,recall_score, f1_score,classification_report,accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred # accuracy\n",
        "cm = confusion_matrix(y_true, y_pred)  # confusion matrix\n",
        "precision = precision_score(y_true, y_pred) # precision\n",
        "recall = recall_score(y_true, y_pred) # recall\n",
        "f1 = f1_score(y_true, y_pred) # f1_score\n",
        "\n",
        "report = classification_report(y_true, y_pred)  # classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7263afa-732d-4cec-b45d-fc5bab481999",
      "metadata": {
        "id": "a7263afa-732d-4cec-b45d-fc5bab481999"
      },
      "source": [
        "### Softmax Logistic Regression\n",
        "Definition: Softmax logistic regression is an extension of logistic regression used for **multi-class** classification problems. It applies the softmax function to predict the probability distribution across multiple classes, ensuring the output probabilities sum to 1.\n",
        "\n",
        "#### hyperparameter\n",
        "multi_class{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac8f255-fa45-47ec-8694-dfeebb165971",
      "metadata": {
        "id": "eac8f255-fa45-47ec-8694-dfeebb165971"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "plot_decision_regions(x.values,y.values,model,legend=2) # model must be classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35471aca-a587-4d2c-96be-49b9d1c992b9",
      "metadata": {
        "id": "35471aca-a587-4d2c-96be-49b9d1c992b9"
      },
      "source": [
        "### Polynomial Logistic Regression\n",
        "Definition: Polynomial Logistic Regression extends logistic regression by adding polynomial features to capture non-linear relationships. This can be done within a pipeline by combining PolynomialFeatures and LogisticRegression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c823e5-ecf9-4ab2-8c98-843a0780c53a",
      "metadata": {
        "id": "79c823e5-ecf9-4ab2-8c98-843a0780c53a"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the pipeline with polynomial features and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('poly_features', PolynomialFeatures(degree=2,include_bias=True)),  # Add polynomial features\n",
        "    ('log_reg', LogisticRegression())  # Logistic regression model\n",
        "])\n",
        "# Fit the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "# Predict using the pipeline\n",
        "y_pred = pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f72ba00-300d-4b4a-b708-0c98d1222381",
      "metadata": {
        "id": "9f72ba00-300d-4b4a-b708-0c98d1222381"
      },
      "source": [
        "## Non - Linear Models\n",
        "### Decision Tree\n",
        "Definition: A decision tree is a non-parametric supervised learning algorithm used for classification and regression. It splits the data into subsets based on feature values, creating a tree-like structure to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf3ac9dc-3d7b-4992-84e9-5994dd445d8c",
      "metadata": {
        "id": "cf3ac9dc-3d7b-4992-84e9-5994dd445d8c"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='entropy or gini',max_depth=12,min_samples_split=12,\n",
        "                              min_samples_leaf=2,max_features=0.5,spliter='best or random')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "# plot a tree\n",
        "plt.figure(figsize(10,5))\n",
        "plot_tree(model,filled=True,feature_names=data.feature_names,class_names=data.target_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f5a555f-fa0d-453f-8d16-d9d14301b74c",
      "metadata": {
        "id": "2f5a555f-fa0d-453f-8d16-d9d14301b74c"
      },
      "source": [
        "### Ensemble Learning\n",
        "Definition: Ensemble learning combines predictions from multiple models to improve overall performance, reduce overfitting, and enhance generalization.\n",
        "#### 1. Voting Ensemble\n",
        "Definition: Combines predictions from multiple models (classifiers or regressors) by voting (for classification) or averaging (for regression). Two types:\n",
        "Hard Voting: Majority voting on predicted classes.\n",
        "Soft Voting: Weighted average of predicted probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f61b8a7-b2f1-4fb1-b27d-5101a8469152",
      "metadata": {
        "id": "9f61b8a7-b2f1-4fb1-b27d-5101a8469152"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier , VotingRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create individual models\n",
        "model1 = LogisticRegression()\n",
        "model2 = DecisionTreeClassifier()\n",
        "model3 = SVC(probability=True)\n",
        "estimators = [('lr', model1), ('dt', model2), ('svc', model3)]\n",
        "# Combine models using VotingClassifier\n",
        "voting_ensemble = VotingClassifier(estimators=estimators, voting='soft', weights=[3,2,1])  # Use 'hard' for majority voting\n",
        "voting_ensemble.fit(X_train, y_train)\n",
        "y_pred = voting_ensemble.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf8af56-8239-44a9-a152-d50b5e303343",
      "metadata": {
        "id": "6bf8af56-8239-44a9-a152-d50b5e303343"
      },
      "source": [
        "#### 2. Bagging Ensemble\n",
        "Definition: Combines predictions by training multiple models on different subsets of the data (sampled with replacement). Each model is trained independently, and their results are averaged (regression) or voted (classification).\n",
        "\n",
        "1. n_estimators: int, default=10    **The number of base estimators in the ensemble.**\n",
        "2. max_samples: int or float, default=1.0 **The number of samples**\n",
        "3. max_features: int or float, default=1.0  **The number of features to draw from X to train**\n",
        "4. bootstrap: bool, default=True  **Whether samples are drawn with replacement. If False, sampling without replacement is performed.**\n",
        "5. oob_score: bool, default=False **Whether to use out-of-bag samples to estimate the generalization error. Only available if bootstrap=True.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021eb58a-ef09-47f6-893b-7bb3b4e04dc3",
      "metadata": {
        "id": "021eb58a-ef09-47f6-893b-7bb3b4e04dc3"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()\n",
        "# Bagging with Decision Trees\n",
        "bagging_ensemble = BaggingClassifier(estimator=model,n_estimators=10, random_state=42)\n",
        "## Pasting  when bootstrap= False\n",
        "\n",
        "bagging_ensemble.fit(X_train, y_train)\n",
        "y_pred = bagging_ensemble.predict(X_test)\n",
        "bagging_ensemble.oob_score_ # Out of bag : when oob_score= True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c03612e-715f-4ae6-a4d6-3090506f283f",
      "metadata": {
        "id": "5c03612e-715f-4ae6-a4d6-3090506f283f"
      },
      "source": [
        "##### Random Forest\n",
        " Def : It is ensemble learning  bagging technique (bootstrap, aggregation) that builds multiple decison tress and combine their predictions to improve accuracy and reduct overfiting.\n",
        " for classification: final prediction is based on majority voting.\n",
        " for regression: final prediction is determined by averaging predictions of all tress.\n",
        "\n",
        " **Hyperparameters** all the decision tree and bagging hyperparameters use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dec45c4-af3b-468b-9269-1be13c2cdf54",
      "metadata": {
        "id": "6dec45c4-af3b-468b-9269-1be13c2cdf54"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators=40,max_depth=5,\n",
        "                           n_jobs=-1,  # model able to use are cores of your pc\n",
        "                          min_samples_split=5,\n",
        "                          bootstrap=True or False)\n",
        "rf.feature_importances_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78fd5482-6323-4151-8344-125efd6a102c",
      "metadata": {
        "id": "78fd5482-6323-4151-8344-125efd6a102c"
      },
      "source": [
        "#### 3. Boosting\n",
        "Definition: Boosting is an ensemble technique that combines weak learners (models that perform slightly better than random guessing) sequentially. Each model corrects the errors of the previous one to improve overall performance.\n",
        "##### AdaBoost ( adaptive Boosting)\n",
        "Def : it is boosting technique that cmobine multiple weak models (classifiers or regressors) (models slighlty better than random guessing) into a strong models to improve overall accuracy.\n",
        "Adaboost focuses on correcting the errors made by previous classifiers by assigning higher weights to misclassified samples in each iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a6c40c-8809-4837-876a-5106a72f6d31",
      "metadata": {
        "id": "73a6c40c-8809-4837-876a-5106a72f6d31"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor()\n",
        "base_model = DecisionTreeRegressor()\n",
        "adaboost = AdaBoostClassifier(estimator=base_model, n_estimators=100,learning_rate=0.1,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gradient Boosting\n",
        "Definition: Gradient Boosting is a boosting technique that builds an ensemble of weak learners (typically decision trees) sequentially, optimizing the residual errors of previous models using gradient descent. It is widely used for both regression and classification tasks."
      ],
      "metadata": {
        "id": "Z6Hml73MyP9v"
      },
      "id": "Z6Hml73MyP9v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4353df-1d1f-4c7e-a060-881281600e5f",
      "metadata": {
        "id": "8d4353df-1d1f-4c7e-a060-881281600e5f"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Gradient Boosting with hyperparameters\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,  # Number of trees\n",
        "    learning_rate=0.1,  # Shrinkage factor\n",
        "    max_depth=3,  # Maximum depth of each tree\n",
        "    subsample=1.0,  # Fraction of samples used per tree\n",
        "    min_samples_split=2,  # Minimum samples to split a node\n",
        "    min_samples_leaf=1,  # Minimum samples in a leaf node\n",
        "    random_state=42  # Randomness control\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred = gb_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### XGBoost\n",
        "Definition: XGBoost (Extreme Gradient Boosting) is an efficient and scalable implementation of gradient boosting designed to optimize speed and performance. It supports regularization, tree pruning, and parallel processing for improved accuracy and reduced overfitting.\n"
      ],
      "metadata": {
        "id": "UcRNIPvb0aha"
      },
      "id": "UcRNIPvb0aha"
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# XGBoost Classifier with hyperparameters\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,  # Number of boosting rounds\n",
        "    learning_rate=0.1,  # Shrinkage factor\n",
        "    max_depth=6,  # Maximum depth of trees\n",
        "    min_child_weight=1,  # Minimum child weight\n",
        "    gamma=0.0,  # Minimum loss reduction for split\n",
        "    subsample=1.0,  # Fraction of samples per tree\n",
        "    colsample_bytree=1.0,  # Fraction of features per tree\n",
        "    reg_alpha=0.0,  # L1 regularization\n",
        "    reg_lambda=1.0,  # L2 regularization\n",
        "    random_state=42  # Random seed\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred = xgb_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "GpK-dhjn0bQn"
      },
      "id": "GpK-dhjn0bQn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ensemble Methods: Stacking and Blending\n",
        "##### 1. Stacking\n",
        "Definition: Stacking combines multiple base models (level-0) and trains a meta-model (level-1) on their predictions. The meta-model learns to predict the target based on the outputs of the base models."
      ],
      "metadata": {
        "id": "K5k1HJ1IyqVG"
      },
      "id": "K5k1HJ1IyqVG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c224fb7-8ff9-4e86-810e-05cc67fdd4dd",
      "metadata": {
        "id": "7c224fb7-8ff9-4e86-810e-05cc67fdd4dd"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Base models\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(max_depth=3)),\n",
        "    ('svc', SVC(probability=True))\n",
        "]\n",
        "\n",
        "# Stacking ensemble\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),  # Meta-model\n",
        "    cv=5  # Cross-validation\n",
        ")\n",
        "stacking_model.fit(X_train, y_train)\n",
        "y_pred = stacking_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. Blending\n",
        "Definition: Similar to stacking, blending combines base models and a meta-model. The difference is that blending uses a validation set (a hold-out subset of the training data) instead of cross-validation to train the meta-model.\n",
        "\n",
        "**Steps**:\n",
        "Split the training data into a training set and a validation set.\n",
        "Train base models on the training set.\n",
        "Generate predictions for the validation set and the test set.\n",
        "Train the meta-model on the validation set predictions.\n",
        "Make final predictions using the meta-model on test set predictions."
      ],
      "metadata": {
        "id": "ChH6yh0xy68T"
      },
      "id": "ChH6yh0xy68T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410e1376-f07f-455e-8b18-1a235343bd03",
      "metadata": {
        "id": "410e1376-f07f-455e-8b18-1a235343bd03"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train_base, X_val, y_train_base, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train base models\n",
        "model1 = DecisionTreeClassifier(max_depth=3).fit(X_train_base, y_train_base)\n",
        "model2 = SVC(probability=True).fit(X_train_base, y_train_base)\n",
        "\n",
        "# Generate validation set predictions\n",
        "val_pred1 = model1.predict_proba(X_val)\n",
        "val_pred2 = model2.predict_proba(X_val)\n",
        "\n",
        "# Concatenate predictions to train meta-model\n",
        "import numpy as np\n",
        "meta_features = np.hstack([val_pred1, val_pred2])\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(meta_features, y_val)\n",
        "\n",
        "# Generate test set predictions using the meta-model\n",
        "test_pred1 = model1.predict_proba(X_test)\n",
        "test_pred2 = model2.predict_proba(X_test)\n",
        "test_meta_features = np.hstack([test_pred1, test_pred2])\n",
        "y_pred = meta_model.predict(test_meta_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Agglomerative Hierarchical Clustering\n",
        "Definition: Agglomerative hierarchical clustering is a bottom-up clustering method that starts with each data point as its own cluster and iteratively merges the closest clusters until a stopping criterion is met (e.g., a desired number of clusters)."
      ],
      "metadata": {
        "id": "HlDH4cF1zLyW"
      },
      "id": "HlDH4cF1zLyW"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Agglomerative clustering with hyperparameters\n",
        "agg_clustering = AgglomerativeClustering(\n",
        "    n_clusters=3,  # Number of clusters\n",
        "    affinity='euclidean',  # Distance metric\n",
        "    linkage='ward'  # Linkage criterion\n",
        ")\n",
        "agg_clustering.fit(X)\n",
        "labels = agg_clustering.labels_\n"
      ],
      "metadata": {
        "id": "Elr4Wh-xzPU2"
      },
      "id": "Elr4Wh-xzPU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K-Nearest Neighbors (KNN)\n",
        "Definition: KNN is a non-parametric, lazy learning algorithm used for classification and regression. It predicts the class or value of a sample based on the majority class or average of its nearest neighbors in the feature space."
      ],
      "metadata": {
        "id": "VRQ3giIKzP8J"
      },
      "id": "VRQ3giIKzP8J"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# KNN with hyperparameters\n",
        "knn_model = KNeighborsClassifier(\n",
        "    n_neighbors=5,  # Number of neighbors\n",
        "    weights='uniform',  # Weight function: uniform or distance\n",
        "    algorithm='auto',  # Algorithm for nearest neighbors computation\n",
        "    leaf_size=30,  # Leaf size for tree-based algorithms\n",
        "    p=2,  # Power parameter for Minkowski distance\n",
        "    metric='minkowski'  # Distance metric\n",
        ")\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred = knn_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "hCL35BNmzRfr"
      },
      "id": "hCL35BNmzRfr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### Naive Bayes\n",
        "Definition: Naive Bayes is a probabilistic classifier based on Bayes' Theorem. It assumes conditional independence between features given the class label. It is efficient and works well for high-dimensional data.\n",
        "\n",
        "**Types**:\n",
        "\n",
        "**Gaussian** Naive Bayes: For continuous data assuming a normal distribution.\n",
        "\n",
        "**Multinomial** Naive Bayes: For discrete data like word counts in text classification.\n",
        "\n",
        "**Bernoulli** Naive Bayes: For binary data."
      ],
      "metadata": {
        "id": "IBoig7Hr0sBc"
      },
      "id": "IBoig7Hr0sBc"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "\n",
        "# Gaussian Naive Bayes\n",
        "gnb = GaussianNB(var_smoothing=1e-9)\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Multinomial Naive Bayes\n",
        "mnb = MultinomialNB(alpha=1.0)\n",
        "mnb.fit(X_train, y_train)\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# Bernoulli Naive Bayes\n",
        "bnb = BernoulliNB(alpha=1.0, binarize=0.0)\n",
        "bnb.fit(X_train, y_train)\n",
        "y_pred = bnb.predict(X_test)\n"
      ],
      "metadata": {
        "id": "QNDU5sNh0ubu"
      },
      "id": "QNDU5sNh0ubu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised ML\n",
        "##### K-Means Clustering\n",
        "Definition: K-Means is a centroid-based clustering algorithm that partitions the dataset into\n",
        "𝑘\n",
        "k clusters by minimizing the variance within each cluster. It iteratively assigns points to clusters and recalculates cluster centroids until convergence."
      ],
      "metadata": {
        "id": "4tD1Lj5lziUj"
      },
      "id": "4tD1Lj5lziUj"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dR75VA4n1Q-j"
      },
      "id": "dR75VA4n1Q-j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# K-Means clustering with hyperparameters\n",
        "kmeans_model = KMeans(\n",
        "    n_clusters=3,  # Number of clusters\n",
        "    init='k-means++',  # Initialization method\n",
        "    n_init=10,  # Number of initializations\n",
        "    max_iter=300,  # Maximum iterations\n",
        "    tol=1e-4,  # Convergence tolerance\n",
        "    algorithm='lloyd'  # Algorithm to use\n",
        ")\n",
        "kmeans_model.fit(X)\n",
        "labels = kmeans_model.labels_\n"
      ],
      "metadata": {
        "id": "1rUM89SPzjDn"
      },
      "id": "1rUM89SPzjDn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
        "Definition: DBSCAN is a density-based clustering algorithm that groups points closely packed together, marking points in low-density regions as noise. It does not require the number of clusters to be specified and can discover clusters of arbitrary shape."
      ],
      "metadata": {
        "id": "8PCFIVtGzjl8"
      },
      "id": "8PCFIVtGzjl8"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# DBSCAN clustering with hyperparameters\n",
        "dbscan_model = DBSCAN(\n",
        "    eps=0.5,  # Maximum distance for neighborhood\n",
        "    min_samples=5,  # Minimum samples to form a core point\n",
        "    metric='euclidean',  # Distance metric\n",
        "    algorithm='auto',  # Nearest neighbor search algorithm\n",
        "    leaf_size=30,  # Leaf size for tree-based algorithms\n",
        "    p=2  # Power parameter for Minkowski metric\n",
        ")\n",
        "dbscan_model.fit(X)\n",
        "labels = dbscan_model.labels_\n"
      ],
      "metadata": {
        "id": "z1iebQBHzkJj"
      },
      "id": "z1iebQBHzkJj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}